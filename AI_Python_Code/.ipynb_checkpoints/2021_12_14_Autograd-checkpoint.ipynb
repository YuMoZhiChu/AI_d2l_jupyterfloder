{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031dc4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor(28., grad_fn=<MulBackward0>) torch.Size([])\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor([True, True, True, True])\n",
      "tensor([1., 1., 1., 1.])\n",
      "y =  tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 2., 4., 6.])\n",
      "torch.ones(len(x)) =  tensor([1., 1., 1., 1.])\n",
      "tensor([0., 2., 4., 6.])\n",
      "u =  tensor([0., 1., 4., 9.]) torch.Size([4])\n",
      "tensor([True, True, True, True])\n",
      "tensor([True, True, True, True])\n",
      "tensor(-0.9410, requires_grad=True)\n",
      "tensor(204800.)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "\n",
    "# 申请一快内存, 去储存 X 的梯度\n",
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True) # 或者是在初始化的时候, 加多一个参数\n",
    "# 现在, X 的梯度信息是存在这里的\n",
    "x.grad  # 默认值是None\n",
    "print(x)\n",
    "\n",
    "# y = 2 <X,X> # y 是 2 倍 x 的内积\n",
    "y = 2 * torch.dot(x,x)\n",
    "# 这里的 y 是一个标量, grad_fn 表示对应的 计算函数(pytorch隐式构造了对应的计算图)\n",
    "# 注意, 这里的标量是指，一个维度\n",
    "print(y, y.shape)\n",
    "\n",
    "# 通过调用反向传播函数, 来自动计算, y 关于 x 每个分量的 梯度\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# 验证是否正确, 因为通过计算 ρy/ρX = 4X\n",
    "print(x.grad == 4*x)\n",
    "\n",
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "# 在 pytorch 中, 下划线表示重写里面的内容\n",
    "# 如果注释掉的话， 就是梯度累加\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "# sum 求导, 是全 1 \n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# 当 y 不是一个标量时，是一个向量时\n",
    "x.grad.zero_()\n",
    "y = x * x # 这就是普通的向量 每个元素 相乘, 结果是一个向量\n",
    "print(\"y = \", y)\n",
    "y.sum().backward()\n",
    "print(x.grad)\n",
    "\n",
    "# 等价于\n",
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "print(\"torch.ones(len(x)) = \", torch.ones(len(x)))\n",
    "# 这里 torch.ones(len(x)) 就是 x 代表的 1 梯度\n",
    "# 所以结果就是  2X, 把向量，当成了一个标量\n",
    "y.backward(torch.ones(len(x)))\n",
    "print(x.grad)\n",
    "\n",
    "# 这里指的是\n",
    "# 虽然我们的定义是 y = x*x 是一个关于 x 的函数, 而且它本事是一个 1 维的内容（向量）\n",
    "# 但是, 我们把 u = y.detach() 就相当于把 它看成一个 值相同, 但是就是值, 不具备梯度信息\n",
    "# 所以 z 关于 x, 是一个维度是 1 的向量, 所以求导是结果是一个 矩阵\n",
    "# 但是我们这里使用 sum() 求梯度, 得到的就是对应的 标量 跟 向量 的求梯度\n",
    "# 得到的就是 u 本身\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "print(\"u = \", u, u.shape)\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "print(x.grad == u)\n",
    "\n",
    "# 同理, 这里 y 是一个 x 的函数, 同样可以去求梯度\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "print(x.grad == 2 * x)\n",
    "\n",
    "\n",
    "# 更厉害的是，对于 python 的控制流中，一样可以进行求导\n",
    "# 在计算的时候，torch 会把计算图给留下来\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "print(a)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "print(a.grad) # 计算当前梯度\n",
    "print(a.grad == d / a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dab5e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], requires_grad=True)\n",
      "y :  tensor(285., grad_fn=<DotBackward0>)\n",
      "x.grad :  tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n",
      "y :  tensor(285., grad_fn=<DotBackward0>)\n",
      "x.grad :  tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36.])\n"
     ]
    }
   ],
   "source": [
    "# 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "import torch\n",
    "x = torch.arange(10.,requires_grad=True)\n",
    "print(\"x : \", x)\n",
    "# 点乘是一个标量\n",
    "y = torch.dot(x**2,torch.ones_like(x))\n",
    "print(\"y : \", y)\n",
    "# backward 求梯度\n",
    "y.backward(retain_graph=True)\n",
    "print(\"x.grad : \", x.grad)\n",
    "print(\"y : \", y)\n",
    "# 再次运行求梯度\n",
    "# 一般来说, 我们不会连续的求两次梯度，因为没有意义\n",
    "# 在后向推导的过程中，因为为了节省内存，会直接在原树上做修改，这样算起来也更方便\n",
    "# 但是如果加上标签 retain_graph 会保持原树图，所以可以再次后向迭代来求导，但就只是多计算了一次相同的内容\n",
    "y.backward()\n",
    "print(\"x.grad : \", x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2607f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 控制流的例子（函数变量），如果把变量 a 改成随机向量，或者是矩阵，看看会发生什么\n",
    "import torch\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045c112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
